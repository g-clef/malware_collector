import os
import requests
from bs4 import BeautifulSoup

from malware_collector import MalwareCollector


class DasMalwerkCollector(MalwareCollector):
    # plan: hit https://dasmalwerk.eu daily, get the list of malware links, find any
    # that we haven't already downloaded, pull those.

    collection_url = "https://das-malwerk.herokuapp.com/"

    def __init__(self):
        super(DasMalwerkCollector, self).__init__()
        self.base_path = self.configfile.get("DasMalwerk", "path")

    def _get_file(self, href, hash_name):
        response = requests.get(href, stream=True)
        with open(os.path.join(self.base_path, hash_name + ".zip"), 'wb') as fileHandle:
            for chunk in response.iter_content(8192):
                fileHandle.write(chunk)

    def get(self):
        already_done = set()
        completed_file_path = os.path.join(self.base_path, ".completed")
        if os.path.exists(completed_file_path):
            with open(completed_file_path) as fileHandle:
                for line in fileHandle:
                    line = line.strip()
                    already_done.add(line)
        response = requests.get(self.collection_url)
        soup = BeautifulSoup(response.content, features='html.parser')
        table = soup.table
        rows = table.find_all("tr")
        did_something = False
        for row in rows:
            elements = row.find_all("td")
            if len(elements) < 3:
                continue
            hash_val = elements[2].text.strip()
            if hash_val not in already_done:
                url = elements[1].a.attrs['href']
                self._get_file(url, hash_val)
                already_done.add(hash_val)
                did_something = True
        if did_something:
            with open(completed_file_path, "w") as fileHandle:
                for entry in already_done:
                    fileHandle.write("{}\n".format(entry))


if __name__ == "__main__":
    malwerk = DasMalwerkCollector()
    malwerk.get()
