import argparse
import base64
import datetime
import hashlib
import logging
import shutil
import os
import tempfile
import time

import requests


from malware_collector import MalwareCollector


logger = logging.getLogger(__name__)


class URLHausSource(MalwareCollector):
    # TODO: make a dynamic user-agent that appears up to date. For now, the user-agent process is so messy, skipping
    ua_string = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36"  # noqa
    everything_url = "https://urlhaus.abuse.ch/downloads/text/"
    thirty_day_url = "https://urlhaus.abuse.ch/downloads/text_recent/"

    def __init__(self):
        super().__init__()
        self.path: str = os.environ.get("URLHAUS_PATH", "/RAID/")
        self.archive_path = os.environ.get("URLHAUS_ARCHIVE_PATH", "urlhaus")
        self.last_urls_collected = set()

    def get_targets(self, url):
        response = requests.get(url)
        if response.status_code != 200:
            raise Exception(f"Error getting target list from urlhaus {response.content}")
        for line in response.iter_lines():
            if line:
                line = line.decode("utf-8")
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                yield line

    def get_everything(self):
        logger.info("Get everything beginning")
        with tempfile.TemporaryDirectory() as tempdir:
            for url in self.get_targets(self.everything_url):
                try:
                    self.process_target(url, tempdir)
                except Exception:
                    logger.exception(f"error processing url {url}")

    def get_recent(self):
        logger.info("get recent beginning")
        urls_processed = set()
        with tempfile.TemporaryDirectory() as tempdir:
            for url in self.get_targets(self.thirty_day_url):
                if url in self.last_urls_collected:
                    urls_processed.add(url)
                    continue
                try:
                    self.process_target(url, tempdir)
                except Exception:
                    logger.exception(f"error processing {url}")
                urls_processed.add(url)
        self.last_urls_collected = urls_processed

    def process_target(self, url, tempdir):
        logger.info(f"getting {url}")
        hash = hashlib.sha256()
        filename = base64.b64encode(url.encode("utf-8")).decode("utf-8")
        outpath = os.path.join(tempdir, filename)
        with open(outpath, "wb") as outfile:
            headers = {"User-Agent": self.ua_string}
            try:
                response = requests.get(url, headers=headers, timeout=1)
                for chunk in response.iter_content(128*1024):
                    hash.update(chunk)
                    outfile.write(chunk)
            except requests.exceptions.Timeout:
                logger.info(f"timeout pulling {url}")
                return
            except Exception:
                logger.exception(f"Error raised getting malware url {url}")
                return
        hex_name = hash.hexdigest()
        new_final_name = f"{hex_name}-{filename}"
        today = datetime.datetime.now(datetime.timezone.utc)

        daydirname = os.path.join(self.path, str(today.year), str(today.month), str(today.day))
        if not os.path.isdir(daydirname):
            os.makedirs(daydirname)
        new_full_path = os.path.join(daydirname, new_final_name)
        logger.info(f"moving file to {new_full_path}")
        shutil.move(outpath, new_full_path)

    def cleanup(self):
        self.get_everything()

    def get(self):
        while True:
            self.get_recent()
            # it's theoretically updated every 5 minutes, but that seems abusive. Let's do every 30
            time.sleep(60*30)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-a","--all", help="get all files", action="store_true")
    arguments = parser.parse_args()
    urlhaus = URLHausSource()
    if arguments.all:
        urlhaus._cleanup = True
    if os.environ.get("URLHAUS_GET_ALL", "False").lower() in ("true", "1", "yes"):
        urlhaus._cleanup = True
    urlhaus.run()
