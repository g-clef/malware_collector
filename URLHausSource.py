import argparse
import datetime
import hashlib
import logging
import shutil
import os
import tempfile
import time

import requests

from stoq import Stoq, RequestMeta
from malwaretl_stoq_transformer import transformer

from malware_collector import MalwareCollector


logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class URLHausSource(MalwareCollector):
    # TODO: make a dynamic user-agent that appears up to date. For now, the user-agent process is so messy, skipping
    ua_string = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36"  # noqa
    everything_url = "https://urlhaus.abuse.ch/downloads/text/"
    thirty_day_url = "https://urlhaus.abuse.ch/downloads/text_recent/"

    def __init__(self, stoq: Stoq, metadata: RequestMeta):
        super().__init__()
        self.path: str = os.environ.get("URLHAUS_PATH", "/RAID")
        self.last_urls_collected = set()
        self.stoq = stoq
        self.metadata = metadata

    def get_targets(self, url):
        response = requests.get(url, timeout=60)
        if response.status_code != 200:
            raise Exception(f"Error getting target list from urlhaus {response.content}")
        for line in response.iter_lines():
            if line:
                line = line.decode("utf-8")
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                yield line

    def get_everything(self):
        logger.info("Get everything beginning")
        with tempfile.TemporaryDirectory() as tempdir:
            for url in self.get_targets(self.everything_url):
                try:
                    self.process_target(url, tempdir)
                except Exception:
                    logger.exception(f"error processing url {url}")

    def get_recent(self):
        logger.info("get recent beginning")
        urls_processed = set()
        with tempfile.TemporaryDirectory() as tempdir:
            for url in self.get_targets(self.thirty_day_url):
                if url in self.last_urls_collected:
                    urls_processed.add(url)
                    continue
                try:
                    saved_path = self.process_target(url, tempdir)
                    if saved_path:
                        self.metadata.extra_data['source_url'] = url
                        self.metadata.extra_data['collection_time'] = datetime.datetime.utcnow().isoformat()
                        self.scan_single_file(stoq, self.metadata, saved_path)
                except Exception:
                    logger.exception(f"error processing {url}")
                urls_processed.add(url)
        self.last_urls_collected = urls_processed

    def process_target(self, url, tempdir) -> str:
        logger.info(f"getting {url}")
        hash256 = hashlib.sha256()
        with tempfile.NamedTemporaryFile(mode="rb+", dir=tempdir) as outfile:
            headers = {"User-Agent": self.ua_string}
            try:
                response = requests.get(url, headers=headers, timeout=1)
                for chunk in response.iter_content(128*1024):
                    hash256.update(chunk)
                    outfile.write(chunk)
            except requests.exceptions.Timeout:
                logger.info(f"timeout pulling {url}")
                return ""
            except requests.exceptions.ConnectionError:
                logger.info(f"error connecting to {url}")
                return ""
            except requests.exceptions.RequestException:
                logger.info(f"Other requests exception connecting to {url}")
            except Exception:
                logger.exception(f"Error raised getting malware url {url}")
                return ""
            hex_name = hash256.hexdigest()
            daydirname = self.make_day_directory()
            done = False
            counter = 0
            while not done:
                new_full_path = os.path.join(daydirname, hex_name + f"___{counter}")
                if os.path.exists(new_full_path):
                    counter += 1
                    continue
                else:
                    done = True
            with open(new_full_path, "wb") as final_file:
                outfile.seek(0)
                final_file.write(outfile.read())
            return new_full_path

    def cleanup(self):
        self.get_everything()

    def get(self):
        while True:
            self.get_recent()
            # it's theoretically updated every 5 minutes, but that seems abusive. Let's do every 30
            time.sleep(60*30)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-a","--all", help="get all files", action="store_true")
    arguments = parser.parse_args()
    stoq, metadata = transformer.init_urlhaus(scan_mode=True)
    urlhaus = URLHausSource(stoq, metadata)
    if arguments.all:
        urlhaus._cleanup = True
    if os.environ.get("URLHAUS_GET_ALL", "False").lower() in ("true", "1", "yes"):
        urlhaus._cleanup = True
    urlhaus.run()
